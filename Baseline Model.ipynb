{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7d4ab9-ed5c-4f72-b9b1-f8c313656c81",
   "metadata": {},
   "source": [
    "## Baseline model for TF-IDF and NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4261aab3-3614-4ef5-8c67-d37e1a86bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time  \n",
    "import re   \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716acb6a-cbae-46c5-946a-296ef722624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 100000 rows from complaints.csv.\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "# I'm using 100k sample for baseline\n",
    "filename = 'complaints.csv' \n",
    "try:\n",
    "    df = pd.read_csv(filename, nrows=100000)\n",
    "    print(f\"Successfully loaded {len(df)} rows from {filename}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{filename}' not found.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f8ac4a-fefa-4a71-ba92-d849b661c5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows remaining after dropping missing narratives: 6868\n",
      "Cleaning and merging duplicate categories...\n",
      "Category merging complete.\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing Data (Using EDA Findings)\n",
    "\n",
    "feature_column = 'Consumer complaint narrative'\n",
    "label_column = 'Product'\n",
    "\n",
    "# Dropping rows where the narrative is missing\n",
    "df.dropna(subset=[feature_column], inplace=True)\n",
    "print(f\"Rows remaining after dropping missing narratives: {len(df)}\")\n",
    "\n",
    "\n",
    "# Merging duplicate categories\n",
    "print(\"Cleaning and merging duplicate categories...\")\n",
    "credit_categories = [\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports',\n",
    "    'Credit reporting or other personal consumer reports',\n",
    "    'Credit reporting'\n",
    "]\n",
    "clean_name = 'Credit Reporting' \n",
    "df[label_column] = df[label_column].replace(credit_categories, clean_name)\n",
    "print(\"Category merging complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b64bc8d-4b13-4fe6-b269-a469ca74e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing PII redaction 'xxxx' tokens from narratives\n",
      "Redaction cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "# Cleaning \"xxxx\" from X\n",
    "print(\"Removing PII redaction 'xxxx' tokens from narratives\")\n",
    "# This regex replaces any word made *only* of 'x' or 'X' with a space.\n",
    "X = df[feature_column].replace(r'\\b[xX]+\\b', ' ', regex=True)\n",
    "y = df[label_column]\n",
    "print(\"Redaction cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d44082b-f7c2-45fa-afea-f2c724adde51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting into training and testing \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m      3\u001b[0m     X, y,\n\u001b[0;32m      4\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,    \n\u001b[0;32m      5\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,  \n\u001b[0;32m      6\u001b[0m     stratify\u001b[38;5;241m=\u001b[39my        \u001b[38;5;66;03m# to maintain class balance in train and test data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Testing set size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal categories in y_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y_train\u001b[38;5;241m.\u001b[39munique())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2681\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2679\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2681\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2684\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2685\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2686\u001b[0m     )\n\u001b[0;32m   2687\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1749\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1719\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m \n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2150\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2148\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2155\u001b[0m     )\n\u001b[0;32m   2157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   2158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   2161\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# Splitting into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,    \n",
    "    random_state=42,  \n",
    "    stratify=y        # to maintain class balance in train and test data\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}, Testing set size: {len(X_test)}\")\n",
    "print(f\"Total categories in y_train: {len(y_train.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504b05e-0a52-40a5-867a-bf485ab40633",
   "metadata": {},
   "source": [
    "The least populated class in y has only 1 member, meaning that in 100,000-row sample, at least one \"Product\" category has only one single complaint.\n",
    "Hence train_test_split can't split that 1 complaint into both a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ddf7d3-7ad1-4bd4-9192-146aa3819b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding and removing rare classes (n < 2)...\n"
     ]
    }
   ],
   "source": [
    "# Removing rare classes \n",
    "print(\"Finding and removing rare classes (n < 2)...\")\n",
    "\n",
    "# Get the counts for each class in 'y'\n",
    "class_counts = y.value_counts()\n",
    "\n",
    "# Finding the list of classes that have fewer than 2 samples\n",
    "rare_classes = class_counts[class_counts < 2].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8becae5-5c19-45a0-a268-5522c59d4169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 rare classes with only 1 sample: ['Payday loan']\n",
      "Filtered data. Old size: 6868, New size: 6867\n"
     ]
    }
   ],
   "source": [
    "if len(rare_classes) > 0:\n",
    "    print(f\"Found {len(rare_classes)} rare classes with only 1 sample: {rare_classes}\")\n",
    "   \n",
    "    keep_indices = y.isin(rare_classes) == False\n",
    "    \n",
    "    X_filtered = X[keep_indices]\n",
    "    y_filtered = y[keep_indices]\n",
    "    \n",
    "    print(f\"Filtered data. Old size: {len(y)}, New size: {len(y_filtered)}\")\n",
    "else:\n",
    "    print(\"No rare classes found. Proceeding with original data.\")\n",
    "    X_filtered = X\n",
    "    y_filtered = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62846d68-e736-42a8-a2fa-dd330364e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 5493, Testing set size: 1374\n",
      "Total categories in y_train: 16\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training and testing \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y_filtered,  \n",
    "    test_size=0.2,   \n",
    "    random_state=42, \n",
    "    stratify=y_filtered \n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}, Testing set size: {len(X_test)}\")\n",
    "print(f\"Total categories in y_train: {len(y_train.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b5301-419e-43e4-9c96-ab06216aa3cd",
   "metadata": {},
   "source": [
    "# Creating the scikit-learn pipeline\n",
    "This pipeline does two things as planned in the proposal:\n",
    "1. 'tfidf': Converts text to TF-IDF features\n",
    "2. 'nb_classifier': Trains the Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d45eeca3-8e7e-4f66-8308-85da0cddfe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline created:\n",
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_features=5000, stop_words='english')),\n",
      "                ('nb_classifier', MultinomialNB())])\n"
     ]
    }
   ],
   "source": [
    "nb_pipeline = Pipeline([\n",
    "    (\n",
    "        'tfidf',\n",
    "        TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=5000  # Only keep the top 5000 words\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        'nb_classifier',\n",
    "        MultinomialNB()\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"\\nPipeline created:\")\n",
    "print(nb_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61713b-2767-4e7e-89da-0b362a4eda26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
